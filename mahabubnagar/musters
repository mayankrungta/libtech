#! /usr/bin/env python

from bs4 import BeautifulSoup

import sys
sys.path.insert(0, '../')

from wrappers.logger import loggerFetch
from wrappers.sn import driverInitialize,driverFinalize,displayInitialize,displayFinalize


#######################
# Global Declarations
#######################



#############
# Functions
#############

def argsFetch():
  '''
  Paser for the argument list that returns the args list
  '''
  import argparse

  parser = argparse.ArgumentParser(description='Script for crawling, downloading & parsing musters')
  parser.add_argument('-v', '--visible', help='Make the browser visible', required=False, action='store_const', const=1)
  parser.add_argument('-l', '--log-level', help='Log level defining verbosity', required=False)
  parser.add_argument('-t', '--timeout', help='Time to wait before a page loads', required=False)
  parser.add_argument('-b', '--browser', help='Specify the browser to test with', required=False)
  parser.add_argument('-u', '--url', help='Specify the url to crawl', required=False)
  #  parser.add_argument('-j', '--jobcard-number', help='Specify the jobcard no to fetch', required=True)
  #  parser.add_argument('-m', '--mobile-number', help='Specify the mobile number', required=True)
  #  parser.add_argument('-i', '--missed-call-id', help='Specify the ID of missed call', required=True)
  parser.add_argument('-d', '--directory', help='Specify directory to download html file to', required=False)
  parser.add_argument('-q', '--query', help='Query to specify the workset, E.g ... where id=147', required=False)

  args = vars(parser.parse_args())
  return args

def parserFinalize(parser):
  parser.close()


from datetime import timedelta,date

def daterange(start_date, end_date):
  for n in range(int ((end_date - start_date).days)):
    yield start_date + timedelta(n)
    
def crawlMusters(logger, driver, cmd=None, dir=None, url=None, date=None):
  '''
  Crawl the html for the musters
  '''
  if cmd == None:
    cmd="CRAWLING"
    
  if dir == None:
    dir = "./html"

  if url == None:
    url = 'http://www.nrega.telangana.gov.in/Nregs/FrontServlet?requestType=SmartCardreport_engRH&actionVal=debitLoagReport&id=1457@DOP$APOL&type=[DATE]&listType='
    #url = "http://www.nrega.telangana.gov.in/Nregs/FrontServlet?requestType=SmartCardreport_engRH&actionVal=debitLoagReport&id=1457@DOP$APOL&type=01/04/2015&listType="
    #url = "http://www.nrega.telangana.gov.in/"

  if not date:
    date='01/04/2015'
    
  url = url.replace('[DATE]', date)

  logger.info("BEGIN %s..." % cmd)

  logger.info("Command[%s] Directory[%s] URL[%s]" % (cmd, dir, url))

  driver.get(url)
  logger.info("Fetching...[%s]" % url)
  
  driver.get(url)    # A double refresh required for the page to load
  logger.info("Refreshing...[%s]" % url)
  
  html_source = driver.page_source.replace('<head>',
                                           '<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>')
  logger.debug("HTML Fetched [%s]" % html_source)

  bs = BeautifulSoup(html_source, "html.parser")
  tr_list = bs.findAll('tr', attrs={'class':['normalRow', 'alternateRow']})
  logger.debug(str(tr_list))

  for tr in tr_list:
    td = tr.find('td')
    td = td.findNext('td')
    panchayat = td.text.strip()
    logger.info("Panchayat[%s]", panchayat)

    elem = driver.find_element_by_link_text(panchayat)
    elem.click()
    
    filename= dir + '/%s_' % date.replace('/','-') + panchayat + '.html'
    with open(filename, 'w') as html_file:
      logger.info("Writing [%s]" % filename)
      html_file.write(driver.page_source.encode('utf-8'))

    driver.back()

  logger.info("...END %s" % cmd)     


def main():
  args = argsFetch()
  logger = loggerFetch(args.get('log_level'))
  logger.info('args: %s', str(args))

  display = displayInitialize(args['visible'])
  driver = driverInitialize(args['browser'])

  outdir = args['directory']
  url = args['url']

  start_date = date(2015, 5, 1)
  end_date = date(2015, 8, 19)
  for day in daterange(start_date, end_date):
    crawlMusters(logger, driver, "CRAWLING", outdir, url, day.strftime("%d/%m/%Y"))

  driverFinalize(driver)
  displayFinalize(display)

  exit(0)

  runTestSuite()
  exit(0)

if __name__ == '__main__':
  main()
